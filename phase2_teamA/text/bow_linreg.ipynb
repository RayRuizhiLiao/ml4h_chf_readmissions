{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suzie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_cohort_with_study_paths.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>48h_hf</th>\n",
       "      <th>14d_hf</th>\n",
       "      <th>30d_hf</th>\n",
       "      <th>er_hf</th>\n",
       "      <th>48h</th>\n",
       "      <th>14d</th>\n",
       "      <th>30d</th>\n",
       "      <th>er</th>\n",
       "      <th>study_id</th>\n",
       "      <th>last_dicom_id</th>\n",
       "      <th>mimic_id</th>\n",
       "      <th>path</th>\n",
       "      <th>study_text</th>\n",
       "      <th>word_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10003400</td>\n",
       "      <td>20214994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52437868</td>\n",
       "      <td>2f6a5fc9-40af95f8-d8762332-f56005ea-b5f85cc2</td>\n",
       "      <td>p10003400_s52437868_2f6a5fc9-40af95f8-d8762332...</td>\n",
       "      <td>files/p10/p10003400/s52437868.txt</td>\n",
       "      <td>final report examination chest portable ap ind...</td>\n",
       "      <td>[[10, 5, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011938</td>\n",
       "      <td>22624746</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56362279</td>\n",
       "      <td>c6eed867-d6efb38a-438501f9-9d2506e9-a0c958f8</td>\n",
       "      <td>p10011938_s56362279_c6eed867-d6efb38a-438501f9...</td>\n",
       "      <td>files/p10/p10011938/s56362279.txt</td>\n",
       "      <td>final report indication ___f with h o complex ...</td>\n",
       "      <td>[[9, 6, 4, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011938</td>\n",
       "      <td>23501236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51895247</td>\n",
       "      <td>bf724128-9131f33a-6fd065d5-19041750-9e7f8707</td>\n",
       "      <td>p10011938_s51895247_bf724128-9131f33a-6fd065d5...</td>\n",
       "      <td>files/p10/p10011938/s51895247.txt</td>\n",
       "      <td>final report examination chest pa and lat indi...</td>\n",
       "      <td>[[5, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012853</td>\n",
       "      <td>26369609</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58181999</td>\n",
       "      <td>aac02704-c647b84c-58d5df12-c9857852-e1536bba</td>\n",
       "      <td>p10012853_s58181999_aac02704-c647b84c-58d5df12...</td>\n",
       "      <td>files/p10/p10012853/s58181999.txt</td>\n",
       "      <td>final report examination chest portable ap ind...</td>\n",
       "      <td>[[5, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10015931</td>\n",
       "      <td>28157142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57792054</td>\n",
       "      <td>094f66e7-fc7faa70-d5f0b6b5-5a8b82f6-58d284b8</td>\n",
       "      <td>p10015931_s57792054_094f66e7-fc7faa70-d5f0b6b5...</td>\n",
       "      <td>files/p10/p10015931/s57792054.txt</td>\n",
       "      <td>wet read ___ ___ ___ 10 53 pm moderate bilater...</td>\n",
       "      <td>[[6, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id  48h_hf  14d_hf  30d_hf  er_hf  48h  14d  30d  er  \\\n",
       "0    10003400  20214994       0       0       0      0    0    0    0   0   \n",
       "1    10011938  22624746       0       0       0      1    0    0    0   1   \n",
       "2    10011938  23501236       0       0       0      0    0    0    0   1   \n",
       "3    10012853  26369609       0       0       0      0    0    0    0   1   \n",
       "4    10015931  28157142       0       0       1      1    0    0    1   1   \n",
       "\n",
       "   study_id                                 last_dicom_id  \\\n",
       "0  52437868  2f6a5fc9-40af95f8-d8762332-f56005ea-b5f85cc2   \n",
       "1  56362279  c6eed867-d6efb38a-438501f9-9d2506e9-a0c958f8   \n",
       "2  51895247  bf724128-9131f33a-6fd065d5-19041750-9e7f8707   \n",
       "3  58181999  aac02704-c647b84c-58d5df12-c9857852-e1536bba   \n",
       "4  57792054  094f66e7-fc7faa70-d5f0b6b5-5a8b82f6-58d284b8   \n",
       "\n",
       "                                            mimic_id  \\\n",
       "0  p10003400_s52437868_2f6a5fc9-40af95f8-d8762332...   \n",
       "1  p10011938_s56362279_c6eed867-d6efb38a-438501f9...   \n",
       "2  p10011938_s51895247_bf724128-9131f33a-6fd065d5...   \n",
       "3  p10012853_s58181999_aac02704-c647b84c-58d5df12...   \n",
       "4  p10015931_s57792054_094f66e7-fc7faa70-d5f0b6b5...   \n",
       "\n",
       "                                path  \\\n",
       "0  files/p10/p10003400/s52437868.txt   \n",
       "1  files/p10/p10011938/s56362279.txt   \n",
       "2  files/p10/p10011938/s51895247.txt   \n",
       "3  files/p10/p10012853/s58181999.txt   \n",
       "4  files/p10/p10015931/s57792054.txt   \n",
       "\n",
       "                                          study_text  \\\n",
       "0  final report examination chest portable ap ind...   \n",
       "1  final report indication ___f with h o complex ...   \n",
       "2  final report examination chest pa and lat indi...   \n",
       "3  final report examination chest portable ap ind...   \n",
       "4  wet read ___ ___ ___ 10 53 pm moderate bilater...   \n",
       "\n",
       "                                         word_counts  \n",
       "0  [[10, 5, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1...  \n",
       "1  [[9, 6, 4, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,...  \n",
       "2  [[5, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1,...  \n",
       "3  [[5, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...  \n",
       "4  [[6, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,...  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_base_dir = \"C:/Users/suzie/Dropbox (MIT)/Spring 2021/6.871 Machine Learning for Healthcare/mimic-cxr-reports/\"\n",
    "\n",
    "data['study_text'] = data['path'].apply(lambda path: pd.read_csv(report_base_dir+path, sep=\"\\t\", header=None).to_numpy().flatten())\n",
    "data['study_text'] = data['study_text'].apply(lambda text: ' '.join(text)).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final report indication ___f with h o complex seizure disorder related to l frontal avm h o aca aneurysm clipped copd fev ___ transferred to ___ from ___ on ___ where she was admitted on ___ p w dizziness and fall s p vats biopsy of lung hypereosinophilic syndrome new o2 requirement eval for pulm edema or etiology of hypoxemia technique chest pa and lateral comparison chest ct dated ___ findings lines and tubes none lungs the lungs are well inflated and demonstrate increased interstitial markings and haziness in bilateral mid and lower zones surgical sutures project over the left apex pleura there is no pleural effusion or pneumothorax mediastinum there is cardiomegaly and unfolding of the thoracic aorta bony thorax unremarkable impression increased interstitial markings and haziness in bilateral mid and lower zones likely a combination of interstitial process with underlying pulmonary edema there is no lobar consolidation '"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['study_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to lower case, remove non-word characters, remove punctuation\n",
    "data['study_text'] = data['study_text'].str.lower()\n",
    "data['study_text'] = data['study_text'].apply(lambda text: re.sub(r'\\W', ' ', text))\n",
    "data['study_text'] = data['study_text'].apply(lambda text: re.sub(r'\\s', ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### should also remove numbers, __'s from study_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get dict carrying all words across all text\n",
    "\n",
    "reports = data['study_text'].to_numpy()\n",
    "\n",
    "word2count = {}\n",
    "for report in reports:\n",
    "    words = nltk.word_tokenize(report)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['word_counts'] = data['study_text'].apply(lambda text: [pd.value_counts(nltk.word_tokenize(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the top n words, here n = 100\n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
    "\n",
    "final_vec = []\n",
    "for report in reports:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(report):\n",
    "#             vector.append(#number of times word appears in report)\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    final_vec.append(vector)\n",
    "final_vec = np.asarray(final_vec)\n",
    "#returns a 2D array #reports x vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 1 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "final_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2594, 100)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "\n",
    "X = final_vec\n",
    "y = list(data['14d_hf'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suzie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', random_state=0)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_48h_hf = LogisticRegression(random_state = 0, class_weight='balanced')\n",
    "logreg_48h_hf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg_48h_hf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 64%\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(y_pred == y_test)*100\n",
    "print ('Accuracy = %.0f%%' %acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies 48h = 99%, 14d = 93%, 30d_hf = 87%, er_hf = 80%, 14h = 98%, 14d = 85%, 30d = 73%, er = 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.66      0.77       482\n",
      "           1       0.06      0.27      0.10        37\n",
      "\n",
      "    accuracy                           0.64       519\n",
      "   macro avg       0.49      0.47      0.43       519\n",
      "weighted avg       0.86      0.64      0.72       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[320, 162],\n",
       "       [ 27,  10]], dtype=int64)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
